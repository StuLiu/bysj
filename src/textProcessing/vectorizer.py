

import re
import jieba.posseg
import config
import os

from sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer
# ä½¿ç”¨scikit-learnè‡ªå¸¦çš„å‘é‡åŒ–æ“ä½œå‘é‡åŒ–è¯„è®ºå†…å®¹
def tf_idf_vectoring(content_array):
    """
    :param content_array: è¯„è®ºæ–‡æœ¬æ•°ç»„
    :return: æ ¹æ®tf-idfæ–¹æ³•å‘é‡åŒ–åçš„çŸ©é˜µ
    """
    countVectorizer = CountVectorizer()
    x_counts = countVectorizer.fit_transform(content_array)
    tfidf_transformer = TfidfTransformer()
    x_tfidf = tfidf_transformer.fit_transform(x_counts)
    print("TfidfTransform finished,result shape:",x_tfidf.shape)
    return x_tfidf


#     æ ¹æ®æ•°æ®æ€»ç»“å‡ºï¼šè¯„è®ºidé•¿åº¦ã€è¯„è®ºé•¿åº¦ã€â€˜è¯„è®ºè¯â€™æ¯”ä¾‹ã€è‹±æ–‡æ‰€å æ¯”ä¾‹ã€
# åè¯æ‰€å æ¯”ä¾‹ã€åŠ¨è¯æ‰€å æ¯”ä¾‹ã€å½¢å®¹è¯æ‰€å æ¯”ä¾‹ã€emojiå’Œç‰¹æ®Šå­—ç¬¦æ‰€å æ¯”ä¾‹ã€
# æ„Ÿå¹è¯æ‰€å æ¯”ä¾‹ã€æ•°å­—å’Œé‡è¯æ‰€å æ¯”ä¾‹ã€è¯„åˆ†ç­‰çº§ç­‰æ•°æ®ä¸ºè¯„è®ºæ–‡æœ¬ç‰¹å¾ã€‚
from database.signed_comments_dbHandler import SignedCommentsDbHandler
from database.apple_app_dbHandler import AppleAppDbHandler
import numpy as np
class Vectorizer(object):

    def __init__(self):
        jieba.load_userdict(os.path.join(config.DICT_PATH, "user_defined_dict.txt"))
        jieba.load_userdict(os.path.join(config.DICT_PATH, "sogoupinyin_dict.txt"))

        signedCommentsDbHandler = SignedCommentsDbHandler()
        appleAppDbHandler = AppleAppDbHandler()

        self.appId_rating_mean_dict = {}
        appList = appleAppDbHandler.queryAll()
        for app in appList:
            print(app)
            self.appId_rating_mean_dict[app[0]] = app[2]

        signedComments = signedCommentsDbHandler.queryAll()
        self.commentId_comment_dict = {}    # {comment_id:signedComment}
        self.userName_commentIds_dict = {}  # {user_name:[comment_id,comment_id,...]}
        self.content_count_dict = {}        # {content:count}
        for signedComment in signedComments:
            self.commentId_comment_dict[signedComment[1]] = signedComment
            if signedComment[8] in self.userName_commentIds_dict.keys():
                self.userName_commentIds_dict[signedComment[8]].append(signedComment[1])
            else:
                self.userName_commentIds_dict[signedComment[8]] = [signedComment[1]]
            if signedComment[3] in self.content_count_dict.keys():
                self.content_count_dict[signedComment[3]] += 1
            else:
                self.content_count_dict[signedComment[3]] = 1
        print('commentId_comment_dict len',len(self.commentId_comment_dict.keys()))
        print('userName_commentIds_dict len',len(self.userName_commentIds_dict.keys()))
        print('content_count_dict len',len(self.content_count_dict.keys()))
        print('init finish')


    def vectoringOneComment(self, comment_id, title, content, rating, user_name, app_id):
        """
        :param comment_id : è¯„è®ºid
        :param title: è¯„è®ºæ ‡é¢˜
        :param content: è¯„è®ºä¸»è¦å†…å®¹
        :param rating : è¯„åˆ†ç­‰çº§
        :param user_name : ç”¨æˆ·å
        :param app_id : åº”ç”¨çš„id
        :return vectorizedComment : å‘é‡åŒ–åçš„è¯„è®ºå‘é‡
        """

        title_len = len(title)
        content_len = len(content)
        content_isUnique = int(self.content_count_dict[content] == 1)
        comment_rating = rating

        commentsIdOfTheReviewer = self.userName_commentIds_dict[user_name]  #list

        comment_count_of_the_reviewer = len(commentsIdOfTheReviewer)

        ratingList = []
        for commentId in commentsIdOfTheReviewer:
            ratingList.append(self.commentId_comment_dict[commentId][6])

        comment_rating_mean_of_the_reviewer = float(np.average(ratingList))
        comment_rating_offset_of_the_reviewer = abs(rating - comment_rating_mean_of_the_reviewer)/5
        comment_rating_std_dev_of_the_reviewer = float(np.std(ratingList))

        comment_rating_isSame_of_the_reviewer = int(len(ratingList)>1 and len(set(ratingList))==1)

        comment_rating_mean_of_app = self.appId_rating_mean_dict[app_id]
        comment_rating_offset_of_the_app = abs(rating-comment_rating_mean_of_app)/5

        contentDict = {}
        keys = ['Ag', 'a', 'ad', 'an', 'b', 'c', 'dg', 'd', 'e', 'f', 'g', 'h',
                'i', 'j', 'k', 'l', 'm', 'Ng', 'n', 'nr', 'ns', 'nt' , 'nz',
                'o', 'p', 'q', 'r', 's', 'tg', 't', 'u', 'vg', 'v', 'vd', 'vn',
                'w', 'x', 'y', 'z', 'un', 'pl']
        for key in keys:
            contentDict[key] = 0.0
        wordAndFlag_list = jieba.posseg.lcut(re.sub(r'[\s]','', content,0))
        wordsCount = len(wordAndFlag_list)
        for word, flag in wordAndFlag_list:
            # print(word, flag)
            try:
                contentDict[flag] += 1.0
            except Exception:
                pass
        content_word_rate_list = []
        for key in keys:
            content_word_rate_list.append(contentDict[key]/wordsCount)

        vectorizedComment = [
            comment_id,
            title_len,
            content_len,
            content_isUnique,
            comment_rating,
            comment_count_of_the_reviewer,
            comment_rating_mean_of_the_reviewer,
            comment_rating_offset_of_the_reviewer,
            comment_rating_std_dev_of_the_reviewer,
            comment_rating_isSame_of_the_reviewer,
            comment_rating_mean_of_app,
            comment_rating_offset_of_the_app
        ] + content_word_rate_list

        # print(vectorizedComment,len(vectorizedComment))
        return vectorizedComment

# def vectoringComment(comment_id, comment_title, comment_content, comment_rating):
#     """
#     :param comment_id: è¯„è®ºid
#     :param comment_title: è¯„è®ºæ ‡é¢˜
#     :param comment_content: è¯„è®ºä¸»è¦å†…å®¹
#     :param comment_rating:  è¯„åˆ†ç­‰çº§
#     :return vectorizedComment: å‘é‡åŒ–åçš„è¯„è®ºå‘é‡
#     """
#     jieba.load_userdict(os.path.join(config.DICT_PATH, "user_defined_dict.txt"))
#     jieba.load_userdict(os.path.join(config.DICT_PATH, "sogoupinyin_dict.txt"))
#
#     punctuation = r'[ï¼!ã€ï¼Œ,ã€‚ï¼š:ï¼›;ï¼Ÿ?â€˜\'â€™â€œ\"â€â€”â€”ï¼ˆï¼‰\(\)ã€ã€‘\[\]\{\}â€¦â€¦\-~`Â·ã€‹ã€Š<>]'
#     content_w_counter = len(re.findall(punctuation, comment_content))   # æ ‡ç‚¹ç¬¦å·æ•°é‡
#     space = r'[\s]'
#     words = jieba.posseg.lcut(re.sub(space,'',comment_content,0))   # å»å¤„ç©ºç™½å­—ç¬¦å†åˆ†è¯
#     words_len = len(words)
#     content_pl_counter = 0      # è¯„è®ºè¯æ•°é‡
#     content_eng_counter = 0     # è‹±æ–‡æ•°é‡
#     content_n_counter = 0       # åè¯æ•°é‡
#     content_v_counter = 0       # åŠ¨è¯æ•°é‡
#     content_a_counter = 0       # å½¢å®¹è¯æ•°é‡
#     content_x_counter = 0 - content_w_counter   # jiebaè§†æ ‡ç‚¹ç¬¦å·ä¸ºxç±»å­—ç¬¦ï¼Œ'.'é™¤å¤–
#     content_y_counter = 0       # è¯­æ°”è¯æ•°é‡
#     content_m_counter = 0       # æ•°è¯æ•°é‡
#     for word, flag in words:
#         # print('%s %s' % (word, flag))
#         if flag == 'pl':
#             content_pl_counter += 1
#         elif flag == 'eng':
#             content_eng_counter += 1
#         elif flag == 'n' or flag == 'nr' or flag == 'ns' or flag == 'nt' or flag == 'nz':
#             content_n_counter += 1
#         elif flag == 'vg' or flag == 'v' or flag == 'vd' or flag == 'vn':
#             content_v_counter += 1
#         elif flag == 'Ag' or flag == 'a' or flag == 'ad' or flag == 'an':
#             content_a_counter += 1
#         elif flag == 'x':
#             content_x_counter += 1
#         elif flag == 'y':
#             content_y_counter += 1
#         elif flag == 'm':
#             content_m_counter += 1
#         else:
#             pass
#     vectorizedComment = [
#         comment_id,
#         len(comment_title),
#         len(comment_content),
#         float(content_pl_counter) / words_len,
#         float(content_eng_counter) / words_len,
#         float(content_n_counter) / words_len,
#         float(content_v_counter) / words_len,
#         float(content_a_counter) / words_len,
#         float(content_x_counter) / words_len,
#         float(content_w_counter) / words_len,
#         float(content_y_counter) / words_len,
#         float(content_m_counter) / words_len,
#         comment_rating
#     ]
#     # print(vectorizedComment)
#     return vectorizedComment


# å»ºç«‹è¯æ±‡è¡¨å¹¶å°†å°†è¯„è®ºå†…å®¹å‘é‡åŒ–
def vectoringContent(max_sequence_len, contentMatrix):
    """
    :param max_sequence_len: å•ä¸ªè¯„è®ºæœ€å¤§è¯è¯­æ•°ï¼Œè¯„è®ºè¯è¯­æ•°å¤§äºæ­¤å€¼æ—¶æˆªå»å¤šä½™è¯è¯­ï¼Œåä¹‹åˆ™å¡«å……
    :param contentMatrix:[[comment_id,comment_content],...]
    :return:[[comment_id,vectorString],...]
    """
    jieba.load_userdict(os.path.join(config.DICT_PATH, "user_defined_dict.txt"))
    jieba.load_userdict(os.path.join(config.DICT_PATH, "sogoupinyin_dict.txt"))
    result = []
    index = 1
    vocabulary = {"PADDING_WORD" : 0}  # {word:index,word:index,...}
    for pair in contentMatrix:
        vectorString = ''
        wordsList = list(jieba.cut(re.sub(r'[^\u4e00-\u9fa5]', '', pair[1], 0),cut_all=False))
        wordsListLen = len(wordsList)
        for num in range(0,max_sequence_len):
            if num < wordsListLen:
                if wordsList[num] in vocabulary:
                    vectorString += ' ' + str(vocabulary[wordsList[num]])
                else:
                    vocabulary[wordsList[num]] = index
                    vectorString += ' ' + str(index)
                    index += 1
            if num >= wordsListLen:
                vectorString += ' 0'
        result.append([pair[0],vectorString])
    for key in vocabulary.keys():
        print(key,vocabulary[key])
    print("vocabulary length:",len(vocabulary))
    print(result)
    return result

if __name__ == '__main__':

    contents = [
        "iPhone Xèš‚èšèŠ±å‘—æ‰“å¼€åä¸€ç‰‡ç©ºç™½",
        "ğŸ‘ğŸ‘ã€ä»˜ä¸´é—¨ã€‘ã€è”åˆ·é’±åŒ…ã€‘0.28%ç§’åˆ°ï¼0.28%ç§’åˆ°ï¼åˆ†äº«å•†æœºï¼Œå…±äº«æœªæ¥ï¼åˆ†äº«æ”¯ä»˜ï¼æ”¹å˜æœªæ¥ï¼åˆ†æ¶¦ä¸‡16+2ç§’ç»“ç§’ç»“ï¼æ€»éƒ¨å’¨è¯¢185-1652-4888å¾®ä¿¡åŒå·",
        "æ¯æ¬¡æ‰“å¼€å°±æ˜¯æç¤ºæ›´æ–°çƒ¦ä¸çƒ¦ï¼Ÿä¸æ˜¯æ¯ä¸ªç”¨æˆ·éƒ½éœ€è¦å•¥åŒåäºŒä¼˜æƒ ï¼Œä¸å±‘æ‡‚ä¸ï¼Ÿ",
        "æ£‹ç‰Œæ¸¸æˆå¼€å‘ï¼šL492078507",
        "åªè¦ä¸€éƒ¨æ‰‹æœºğŸ“±ï¼Œèººåœ¨å®¶é‡Œä¹Ÿèƒ½zuan0ğŸŒ¸ğŸ’°â•ä¸ºâ¤615912134",
        "æ”¯ä»˜å®æ¨èï¼å…³æ³¨VXå…±ä¼—è™Ÿã€swapp321ã€‘æ‰‹æœ¨å‡ è½¬RMï¿¥ï¼Œå½“å¤©è¿›å£è¢‹è´°ä½°åœ†ï¼",
        "rt æ— åŠ›åæ§½sbé©¬äº‘",
        "    s  è†œæ‹œå•è½¦æ‘©æ‹œå•è½¦â¤â¤ ",
        "Very useful!thanks a lot",
        "å•Šå•Šå•Šå•Šå•Šå•Šå•Šå•Šå•Šå•Šå•Šå•Šå“¦å•Šå•Šå•Šå•Šå‘€å‘€å‘€å‘ƒå‘ƒå‘ƒå“¦å“¦å“¦å’¦55555555å’¦",
        "!?ï¼Ÿã€‚ã€\'\"{}[]ã€ã€‘.å•Š"
    ]
    # contentMatrix = [
    #     ["123456", "iPhone Xèš‚èšèŠ±å‘—æ‰“å¼€åä¸€ç‰‡ç©ºç™½"],
    #     ["123457", "ğŸ‘ğŸ‘ã€ä»˜ä¸´é—¨ã€‘ã€è”åˆ·é’±åŒ…ã€‘0.28%ç§’åˆ°ï¼0.28%ç§’åˆ°ï¼åˆ†äº«å•†æœºï¼Œå…±äº«æœªæ¥ï¼åˆ†äº«æ”¯ä»˜ï¼æ”¹å˜æœªæ¥ï¼åˆ†æ¶¦ä¸‡16+2ç§’ç»“ç§’ç»“ï¼æ€»éƒ¨å’¨è¯¢185-1652-4888å¾®ä¿¡åŒå·"],
    #     ["123458", "æ¯æ¬¡æ‰“å¼€å°±æ˜¯æç¤ºæ›´æ–°çƒ¦ä¸çƒ¦ï¼Ÿä¸æ˜¯æ¯ä¸ªç”¨æˆ·éƒ½éœ€è¦å•¥åŒåäºŒä¼˜æƒ ï¼Œä¸å±‘æ‡‚ä¸ï¼Ÿ   "]
    # ]
    # vectoringContent(25, contentMatrix)

    v = Vectorizer()